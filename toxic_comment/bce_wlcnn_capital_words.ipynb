{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSUES #\n",
    "## validation set에 대해 계산한 Columnwise mean ROC AUC가 실제 테스트셋에 대해 제출했을 때 값과 차이가 많이 남"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- pytorch\n",
    "- torchtext\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- tqdm\n",
    "- gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 설명\n",
    "\n",
    "- embedding layer\n",
    "- |\n",
    "- convolutional layer (kernel = 3 x embedding dim)\n",
    "- |\n",
    "- leakyrelu\n",
    "- |\n",
    "- dropout\n",
    "- |\n",
    "- maxpool w.r.t time axis\n",
    "- |\n",
    "- fcn1 for each labels\n",
    "- |\n",
    "- fcn2 for each labels ( -> label output )\n",
    "- | \n",
    "- sigmoid - BinaryCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 len_sentence,\n",
    "                 channel_size=4,\n",
    "                 x2_size=1, # additional data - cap ratio\n",
    "                 fc_dim=128,\n",
    "                 padding_idx=1,\n",
    "                 dropout=0.3,\n",
    "                 num_labels=7,\n",
    "                 batch_size=32,\n",
    "                 is_cuda=False,\n",
    "                 n_gram=5,\n",
    "                 additional_kernel_size=1,\n",
    "                ):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size+2, embedding_dim=embedding_dim, padding_idx=padding_idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.channel_size = channel_size\n",
    "        self.len_sentence = len_sentence\n",
    "        self.batch_size = batch_size\n",
    "        self.x2_size = x2_size\n",
    "        self.kernel_size = (n_gram, embedding_dim + additional_kernel_size)\n",
    "        self.n_gram = n_gram\n",
    "        self.additional_kernel_size = additional_kernel_size\n",
    "        self.conv2d = nn.Conv2d(1, out_channels=channel_size, kernel_size=self.kernel_size, stride=1)\n",
    "        # output : batch x channel x (len_sentence - 2) x 1\n",
    "        \n",
    "        # -> squeeze : batch x channel x (len_sentence - 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout1d = nn.Dropout(p=dropout)\n",
    "        self.pool1d = nn.AvgPool1d(kernel_size=2)\n",
    "        # output : batch x channel x (len_sentence - 2) / 2\n",
    "        \n",
    "        self.bottleneck_size = channel_size * (len_sentence - (self.n_gram-1)) / 2\n",
    "        assert self.bottleneck_size.is_integer()\n",
    "        self.bottleneck_size = int(self.bottleneck_size) + self.x2_size\n",
    "        \n",
    "        self.fcn1 = nn.Linear(self.bottleneck_size, fc_dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fcn2 = nn.Linear(fc_dim, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_dim = fc_dim\n",
    "        self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, sentence, cap_ratio, other_features):\n",
    "#         print(\"sentence \", sentence.shape)\n",
    "        image = self.embedding(sentence)\n",
    "#         print(bottleneck.shape)\n",
    "#         image.unsqueeze_(1)\n",
    "#         print(\"image \", image.shape)\n",
    "        # batch x channel x sentence_length x embedding\n",
    "        cap_ratio.unsqueeze_(2)\n",
    "#         print(\"cap_Ratio :\", cap_ratio.shape)\n",
    "        new_image = torch.cat([image, cap_ratio], dim=2)\n",
    "        new_image.unsqueeze_(1)\n",
    "        bottleneck = self.conv2d(new_image)\n",
    "        bottleneck.squeeze_(3)\n",
    "        bottleneck = self.relu(bottleneck) # batch x channel x features\n",
    "        bottleneck = self.dropout1d(bottleneck)\n",
    "        bottleneck = self.pool1d(bottleneck)\n",
    "#         print(\"bt shape \", bottleneck.shape)\n",
    "        \n",
    "        bottleneck = bottleneck.view(-1, self.bottleneck_size - self.x2_size)\n",
    "        if self.x2_size > 0:\n",
    "            bottleneck = torch.cat([bottleneck, other_features], dim=1)\n",
    "        \n",
    "        fcn = self.relu1(self.fcn1(bottleneck))\n",
    "        fcn = self.fcn2(fcn)\n",
    "        logit = self.sigmoid(fcn)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 20000\n",
    "    embedding_dim = 100 # TODO: max 300\n",
    "    len_sentence = 100\n",
    "    num_labels = 6\n",
    "    min_freq = 1\n",
    "    batch_size = 64\n",
    "    channel_size = 128\n",
    "    seed = 0\n",
    "    dropout = 0.5 # TODO: batch norm으로 대체 추천\n",
    "    x2_size = 1\n",
    "    valid_num = 10000\n",
    "    n_gram = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# seed 고정\n",
    "torch.cuda.manual_seed_all(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_pd_data(path : str):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train = get_pd_data('./data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test = get_pd_data('./data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (1)\n",
    "----\n",
    "###  Set captial character ratio\n",
    "- 문장 내의 대문자 비율을 뉴럴넷의 input으로 줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def set_capital_ratio(df : pd.DataFrame):\n",
    "    df['alphas'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isalpha()))\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['cap_ratio'] = df.apply(lambda row: float(row['capitals']) / (float(row['alphas']) + 1), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set_capital_ratio(train), set_capital_ratio(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess(2)\n",
    "-----\n",
    "### Word tokenize\n",
    "- gensim의 tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(string : str):\n",
    "    return [s for s in simple_tokenize(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cap_ratio_of_words(tokens : list):\n",
    "    return [(sum(1 for c in token if c.isupper()))/(sum(1 for c in token if c.isalpha()) + 1) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tk_train = train['comment_text'].apply(tokenizer)\n",
    "tk_test = train['comment_text'].apply(tokenizer)\n",
    "tk_cap_ratio_train = tk_train.apply(get_cap_ratio_of_words)\n",
    "tk_cap_ratio_test = tk_test.apply(get_cap_ratio_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tk_train = train['comment_text'].str.lower().apply(tokenizer)\n",
    "tk_test = train['comment_text'].str.lower().apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tk_cap_ratio_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tk_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "x_features = ['cap_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess(3)\n",
    "----\n",
    "### Add Normal column label\n",
    "- toxic하지 않은 label로 분류되는 것에, normal=1 의 새로운 라벨 추가 (하지않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train['normal'] = 0\n",
    "# train.loc[train[labels].sum(axis=1) == 0, 'normal'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels.append('normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_labels = train[labels]\n",
    "y_labels.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add manually engineered features ex) capital ratio of sentence\n",
    "x2 = train[x_features]\n",
    "x2.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint = dict({\n",
    "    'tk_train' : tk_train,\n",
    "    'tk_test' : tk_test,\n",
    "    'tk_cap_ratio_train' : tk_cap_ratio_train,\n",
    "    'tk_cap_ratio_test' : tk_cap_ratio_test,\n",
    "    'y_labels' : y_labels,\n",
    "    'x2' : x2,\n",
    "})\n",
    "torch.save(checkpoint, './saved_state.tch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved = torch.load('./saved_state.tch')\n",
    "tk_train = saved['tk_train']\n",
    "tk_test = saved['tk_test']\n",
    "tk_cap_ratio_train = saved['tk_cap_ratio_train']\n",
    "tk_cap_ratio_test = saved['tk_cap_ratio_test']\n",
    "y_labels = saved['y_labels']\n",
    "x2 = saved['x2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "----\n",
    "### 10000 개의 Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x, y, z : pd.Dataframe of pd.series\n",
    "def shuffle_lists(*pargs):\n",
    "    shuffler = np.random.permutation(len(pargs[0]))\n",
    "    retargs = []\n",
    "    for x in pargs:\n",
    "        x = x.iloc[shuffler]\n",
    "        retargs.append(x)\n",
    "    return retargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding_cap_ratio(tokens : list):\n",
    "    L = len(tokens)\n",
    "    ret = tokens[:min(L, config.len_sentence)]\n",
    "    ret = [1] * (config.len_sentence - L) + ret\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk_train, x2, tk_cap_ratio_train, y_labels = shuffle_lists(tk_train, x2, tk_cap_ratio_train, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk_cap_ratio_train = np.array([padding_cap_ratio(row) for row in tk_cap_ratio_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk_valid = tk_train[-config.valid_num:]\n",
    "tk_cap_ratio_valid = tk_cap_ratio_train[-config.valid_num:]\n",
    "x2_valid = x2[-config.valid_num:]\n",
    "y_valid = y_labels[-config.valid_num:]\n",
    "\n",
    "tk_train = tk_train[:-config.valid_num]\n",
    "y_train = y_labels[:-config.valid_num]\n",
    "x2_train = x2[:-config.valid_num]\n",
    "tk_cap_ratio_train = tk_cap_ratio_train[:-config.valid_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000, 10000, 10000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk_valid), len(tk_cap_ratio_valid), len(x2_valid), len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149571, 149571, 149571, 149571)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk_train), len(tk_cap_ratio_train), len(x2_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess(3)\n",
    "---\n",
    "### torchtext.data.Field\n",
    "- word dictionary, word to index 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,  \n",
    "                  # 들어갈 데이터가 sequential 인가요? 우리는 tokenize한 word의 sequence를 다룰거니까 True입니다. Defualt로도 True임.\n",
    "                  tokenize=tokenizer, \n",
    "                  # 그 데이터를 tokenize할 함수를 지정할 수 있습니다. 우리는 gensim library의 tokenize 함수를 쓸건데요\n",
    "                  # 뭐 굳이 그거 말고도 직접 정의해도 되고 str.split 같은걸 써넣어도 됩니다.\n",
    "                  # :: 그런 줄 알았는데 아무 tokenize 함수나 쓰면 안되고, generator가 아닌 tokenized list 를 반환하는 함수여야합니다..\n",
    "                  # :::: 이게 아닐거같기도 함.\n",
    "                  fix_length=config.len_sentence,\n",
    "                 # 아마 tokenize된 길이 제한 같은데 한번 확인해볼게요. 특이사항으로는 length 넘으면 자르고, 안넘으면 padding을 채웁니다\n",
    "                  # :: 그게 아니고 vector화 했을 때의 길이 제한일 것 같아요. 확인해보겠습니다.\n",
    "                  pad_first=True,\n",
    "                  # padding이 앞에서부터 붙냐, 뒤에서부터 붙냐는 겁니다.\n",
    "                  tensor_type=torch.cuda.LongTensor\n",
    "                  # cuda를 써도 됩니다\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(tk_train, tk_valid, max_size=config.vocab_size, min_freq=config.min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(*pargs, batch_size=32):\n",
    "    for i in range(0, len(pargs[0]), batch_size):\n",
    "        end = min(i+batch_size, len(tk_train))\n",
    "        yield [batch[i:end] for batch in pargs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net(vocab_size=config.vocab_size, embedding_dim=config.embedding_dim, len_sentence=config.len_sentence,\n",
    "         x2_size=config.x2_size, channel_size=config.channel_size, dropout=config.dropout, num_labels=config.num_labels, batch_size=config.batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(20002, 100, padding_idx=1)\n",
       "  (conv2d): Conv2d (1, 128, kernel_size=(5, 101), stride=(1, 1))\n",
       "  (relu): ReLU(inplace)\n",
       "  (dropout1d): Dropout(p=0.5)\n",
       "  (pool1d): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,), ceil_mode=False, count_include_pad=True)\n",
       "  (fcn1): Linear(in_features=6145, out_features=128)\n",
       "  (relu1): ReLU(inplace)\n",
       "  (fcn2): Linear(in_features=128, out_features=6)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "# criterions = [nn.CrossEntropyLoss() for i in range(config.num_labels)]\n",
    "# -> Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(net, tk_valid, tk_cap_ratio_valid, x2_valid, y_valid : pd.DataFrame, TEXT : data.Field, criterion):\n",
    "    net.train(False)\n",
    "    val_score = None\n",
    "    valid_loss = 0 \n",
    "    for val_step, (batch_val, cr_val, x2_val, y_val) in enumerate(batchify(tk_valid, tk_cap_ratio_valid, x2_valid.values, y_valid.values, batch_size=config.batch_size)):\n",
    "        var_x = TEXT.process(batch_val, device=0, train=False).transpose(dim0=0, dim1=1)\n",
    "        var_cr = Variable(torch.cuda.FloatTensor(cr_val))\n",
    "        var_y = Variable(torch.cuda.FloatTensor(y_val))\n",
    "        var_x2 = Variable(torch.cuda.FloatTensor(x2_val))\n",
    "        pred_score = net(var_x, var_cr, var_x2)\n",
    "        if val_score is None:\n",
    "            val_score = pred_score.data\n",
    "        else:\n",
    "            val_score = torch.cat([val_score, pred_score.data])\n",
    "        y_loss = criterion(pred_score, var_y)\n",
    "        valid_loss += y_loss.data[0]\n",
    "    net.train(True)\n",
    "    valid_loss /= len(tk_valid) / config.batch_size\n",
    "            \n",
    "    return valid_loss, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1028it [00:06, 155.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.054999392579495904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2030it [00:13, 155.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.05417662822753191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2338it [00:14, 157.06it/s]\n"
     ]
    }
   ],
   "source": [
    "train_corrects = [0 for i in range(config.num_labels)]\n",
    "train_loss = 0\n",
    "net.train(True)\n",
    "for step, (x_train_, cr_train_, x2_train_, y_train_) in tqdm(enumerate(batchify(tk_train, tk_cap_ratio_train, x2_train.values, y_train.values, batch_size=config.batch_size))):\n",
    "    var_x = TEXT.process(x_train_, device=0, train=True).transpose(dim0=0, dim1=1)\n",
    "    var_cr = Variable(torch.cuda.FloatTensor(cr_train_))\n",
    "    var_y = Variable(torch.cuda.FloatTensor(y_train_))\n",
    "    var_x2 = Variable(torch.cuda.FloatTensor(x2_train_))\n",
    "    pred_score = net(var_x, var_cr, var_x2)\n",
    "    \n",
    "    net.zero_grad()\n",
    "    \n",
    "    \n",
    "    y_loss = criterion(pred_score, var_y)\n",
    "#     if step % 100 == 99:\n",
    "#         print(y_loss.data[0])\n",
    "    y_loss.backward()\n",
    "    if step % 1000 == 999:\n",
    "        valid_loss, val_score = validation(net, tk_valid, tk_cap_ratio_valid, x2_valid, y_valid, TEXT, criterion)\n",
    "        print(\"valid loss\", valid_loss)\n",
    "#         print(\"valid acc\", [i.data[0] for i in valid_acc])\n",
    "\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_loss, val_score = validation(net, tk_valid, tk_cap_ratio_valid, x2_valid, y_valid, TEXT, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "---\n",
    "### roc_auc_score w.r.t. validation set's score\n",
    "- Kaggle form에 맞추어 column-wise roc auc score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966439002321\n",
      "0.987104608907\n",
      "0.983839732223\n",
      "0.96161002333\n",
      "0.978819611599\n",
      "0.95693447737\n"
     ]
    }
   ],
   "source": [
    "roc_auc_scores = 0\n",
    "for i in range(config.num_labels): # minus 1 for normal \n",
    "    score = roc_auc_score( y_valid.values[:, i], val_score.cpu().numpy()[:, i])\n",
    "    print(score)\n",
    "    roc_auc_scores += score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972457909292\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_scores / (config.num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97245790929169551"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_valid.values, val_score.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 0.975129977385 - CHANNEL 128 + PURE RELU between FCN + PURE RELU between CNN + dropout 0.5 + AvgPool1d between CNN + sentence length 100 + embedding 100 + min_freq 1 + kernel size 5 + epoch 3\n",
    "- 0.971308288115 - same above + word capital embedding + epoch 2\n",
    "- 0.97435546273329887 - same above + epoch 3\n",
    "- 0.97201811975211161 - same above + epoch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_recsys]",
   "language": "python",
   "name": "conda-env-tf_recsys-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
