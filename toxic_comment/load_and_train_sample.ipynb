{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- pytorch\n",
    "- torchtext\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- tqdm\n",
    "- gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 설명\n",
    "\n",
    "- embedding layer\n",
    "- |\n",
    "- convolutional layer (kernel = 3 x embedding dim)\n",
    "- |\n",
    "- leakyrelu\n",
    "- |\n",
    "- dropout\n",
    "- |\n",
    "- maxpool w.r.t time axis\n",
    "- |\n",
    "- fcn1 for each labels\n",
    "- | | | | | | |\n",
    "- fcn2 for each labels ( -> binary output )\n",
    "- | | | | | | |\n",
    "- CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 len_sentence,\n",
    "                 channel_size=4,\n",
    "                 fc_dim=128,\n",
    "                 padding_idx=1,\n",
    "                 dropout=0.3,\n",
    "                 num_labels=7,\n",
    "                 batch_size=32,\n",
    "                 is_cuda=False\n",
    "                ):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size+2, embedding_dim=embedding_dim, padding_idx=padding_idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.channel_size = channel_size\n",
    "        self.len_sentence = len_sentence\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.conv2d = nn.Conv2d(1, out_channels=channel_size, kernel_size=(3, embedding_dim), stride=1)\n",
    "        # output : batch x channel x (len_sentence - 2) x 1\n",
    "        # -> squeeze : batch x channel x (len_sentence - 2)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout1d = nn.Dropout(p=dropout)\n",
    "        self.pool1d = nn.MaxPool1d(kernel_size=2)\n",
    "        # output : batch x channel x (len_sentence - 2) / 2\n",
    "        \n",
    "        self.bottleneck_size = channel_size * (len_sentence - 2) / 2\n",
    "#         print (\"Linear size : %sx(%s-2)/2\"%(channel_size, len_sentence), self.bottleneck_size)\n",
    "        assert self.bottleneck_size.is_integer()\n",
    "        self.bottleneck_size = int(self.bottleneck_size)\n",
    "        \n",
    "        self.fcns1 = [nn.Linear(self.bottleneck_size, fc_dim) for i in range(num_labels)]\n",
    "        self.fcns2 = [nn.Linear(fc_dim, 2) for i in range(num_labels)]\n",
    "        \n",
    "        for i, fcn in enumerate(self.fcns1):\n",
    "            self.add_module(\"fcn1-\"+str(i), fcn)\n",
    "        \n",
    "        for i, fcn2 in enumerate(self.fcns2):\n",
    "            self.add_module(\"fcn2-\"+str(i), fcn2)\n",
    "        \n",
    "        self.fc_dim = fc_dim\n",
    "        self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, sentence, other_features=None):\n",
    "#         print(\"sentence \", sentence.shape)\n",
    "        image = self.embedding(sentence)\n",
    "#         print(bottleneck.shape)\n",
    "        image.unsqueeze_(1)\n",
    "#         print(\"image \", image.shape)\n",
    "        \n",
    "        bottleneck = self.conv2d(image)\n",
    "        bottleneck.squeeze_(3)\n",
    "        bottleneck = self.relu(bottleneck)\n",
    "        bottleneck = self.dropout1d(bottleneck)\n",
    "        bottleneck = self.pool1d(bottleneck)\n",
    "#         print(\"bt shape \", bottleneck.shape)\n",
    "        \n",
    "        bottleneck = bottleneck.view(-1, self.bottleneck_size)\n",
    "        fcns_1 = []\n",
    "        for i in range(self.num_labels):\n",
    "            fcns_1.append(self.fcns1[i](bottleneck))\n",
    "        \n",
    "        fcns_2 = []\n",
    "        for i in range(self.num_labels):\n",
    "            fcns_2.append(self.fcns2[i](fcns_1[i]))\n",
    "            \n",
    "        return fcns_2 # return num_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 20000\n",
    "    embedding_dim = 50\n",
    "    len_sentence = 30\n",
    "    num_labels = 7\n",
    "    min_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pd_data(path : str):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = get_pd_data('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = get_pd_data('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (1)\n",
    "----\n",
    "###  Set captial character ratio (not be used now)\n",
    "- 문장 내의 대문자 비율을 나중에 뉴럴넷의 input으로 줄 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_capital_ratio(df : pd.DataFrame):\n",
    "    df['alphas'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isalpha()))\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['cap_ratio'] = df.apply(lambda row: float(row['capitals']) / (float(row['alphas']) + 1), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_capital_ratio(train), set_capital_ratio(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>alphas</th>\n",
       "      <th>capitals</th>\n",
       "      <th>cap_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>17</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>0.108108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "      <td>11</td>\n",
       "      <td>0.022587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  alphas  capitals  \\\n",
       "0             0        0       0       0              0     203        17   \n",
       "1             0        0       0       0              0      73         8   \n",
       "2             0        0       0       0              0     186         4   \n",
       "3             0        0       0       0              0     486        11   \n",
       "4             0        0       0       0              0      50         2   \n",
       "\n",
       "   cap_ratio  \n",
       "0   0.083333  \n",
       "1   0.108108  \n",
       "2   0.021390  \n",
       "3   0.022587  \n",
       "4   0.039216  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess(2)\n",
    "-----\n",
    "### Word tokenize\n",
    "- gensim의 tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(string : str):\n",
    "    return [s for s in simple_tokenize(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk_train = train['comment_text'].str.lower().apply(tokenizer)\n",
    "tk_test = train['comment_text'].str.lower().apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [explanation, why, the, edits, made, under, my...\n",
       "1    [d, aww, he, matches, this, background, colour...\n",
       "2    [hey, man, i, m, really, not, trying, to, edit...\n",
       "3    [more, i, can, t, make, any, real, suggestions...\n",
       "4    [you, sir, are, my, hero, any, chance, you, re...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess(3)\n",
    "----\n",
    "### Add Normal column label\n",
    "- toxic하지 않은 label로 분류되는 것에, normal=1 의 새로운 라벨 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['normal'] = 0\n",
    "train.loc[train[labels].sum(axis=1) == 0, 'normal'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels.append('normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  normal\n",
       "0      0             0        0       0       0              0       1\n",
       "1      0             0        0       0       0              0       1\n",
       "2      0             0        0       0       0              0       1\n",
       "3      0             0        0       0       0              0       1\n",
       "4      0             0        0       0       0              0       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = train[labels]\n",
    "y_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "----\n",
    "### 10000 개의 Validation set\n",
    "    TODO\n",
    "    Validation 나누기 전에 shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk_valid = tk_train[-valid_num:]\n",
    "y_valid = y_labels[-valid_num:]\n",
    "tk_train = tk_train[:-valid_num]\n",
    "y_labels = y_labels[:-valid_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess(3)\n",
    "---\n",
    "### torchtext.data.Field\n",
    "- word dictionary, word to index 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,  \n",
    "                  # 들어갈 데이터가 sequential 인가요? 우리는 tokenize한 word의 sequence를 다룰거니까 True입니다. Defualt로도 True임.\n",
    "                  tokenize=tokenizer, \n",
    "                  # 그 데이터를 tokenize할 함수를 지정할 수 있습니다. 우리는 gensim library의 tokenize 함수를 쓸건데요\n",
    "                  # 뭐 굳이 그거 말고도 직접 정의해도 되고 str.split 같은걸 써넣어도 됩니다.\n",
    "                  # :: 그런 줄 알았는데 아무 tokenize 함수나 쓰면 안되고, generator가 아닌 tokenized list 를 반환하는 함수여야합니다..\n",
    "                  # :::: 이게 아닐거같기도 함.\n",
    "                  fix_length=config.len_sentence,\n",
    "                 # 아마 tokenize된 길이 제한 같은데 한번 확인해볼게요. 특이사항으로는 length 넘으면 자르고, 안넘으면 padding을 채웁니다\n",
    "                  # :: 그게 아니고 vector화 했을 때의 길이 제한일 것 같아요. 확인해보겠습니다.\n",
    "                  pad_first=True,\n",
    "                  # padding이 앞에서부터 붙냐, 뒤에서부터 붙냐는 겁니다.\n",
    "                  tensor_type=torch.cuda.LongTensor\n",
    "                  # cuda를 써도 됩니다\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(tk_train, tk_valid, max_size=config.vocab_size, min_freq=config.min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(tk_train, y_labels, batch_size=32):\n",
    "    for i in range(0, len(tk_train), batch_size):\n",
    "        yield tk_train[i:min(i+batch_size, len(tk_train))], y_labels[i:min(i+batch_size, len(tk_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net(vocab_size=config.vocab_size, embedding_dim=config.embedding_dim, len_sentence=config.len_sentence,\n",
    "         channel_size=8, num_labels=config.num_labels, batch_size=32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(20002, 50, padding_idx=1)\n",
       "  (conv2d): Conv2d (1, 8, kernel_size=(3, 50), stride=(1, 1))\n",
       "  (relu): LeakyReLU(0.01)\n",
       "  (dropout1d): Dropout(p=0.3)\n",
       "  (pool1d): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fcn1-0): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-1): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-2): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-3): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-4): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-5): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-6): Linear(in_features=112, out_features=128)\n",
       "  (fcn2-0): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-1): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-2): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-3): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-4): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-5): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-6): Linear(in_features=128, out_features=2)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "criterions = [nn.CrossEntropyLoss() for i in range(config.num_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(20002, 50, padding_idx=1)\n",
       "  (conv2d): Conv2d (1, 8, kernel_size=(3, 50), stride=(1, 1))\n",
       "  (relu): LeakyReLU(0.01)\n",
       "  (dropout1d): Dropout(p=0.3)\n",
       "  (pool1d): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fcn1-0): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-1): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-2): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-3): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-4): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-5): Linear(in_features=112, out_features=128)\n",
       "  (fcn1-6): Linear(in_features=112, out_features=128)\n",
       "  (fcn2-0): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-1): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-2): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-3): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-4): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-5): Linear(in_features=128, out_features=2)\n",
       "  (fcn2-6): Linear(in_features=128, out_features=2)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(net, tk_valid, y_valid : pd.DataFrame, TEXT : data.Field, criterions : list):\n",
    "    net.train(False)\n",
    "    val_corrects = [0 for i in range(config.num_labels)]\n",
    "    val_score = []\n",
    "    valid_loss = 0 \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    for val_step, (batch_val, y_val) in enumerate(batchify(tk_valid, y_valid.values)):\n",
    "        y_total_loss = 0\n",
    "        var_batch = TEXT.process(batch_val, device=0, train=False)\n",
    "        var_y = Variable(torch.cuda.LongTensor(y_val)).transpose(dim0=0, dim1=1)\n",
    "        pred_score = net(var_batch.transpose(dim0=0, dim1=1))\n",
    "        val_score.append([softmax(score).data.cpu() for score in pred_score])\n",
    "        for i, score in enumerate(pred_score):\n",
    "            _, pred = score.max(dim=1)\n",
    "            val_corrects[i] += (pred == var_y[i]).float().sum()\n",
    "            y_loss = criterions[i](score, var_y[i])\n",
    "            y_total_loss += y_loss\n",
    "        valid_loss += y_total_loss\n",
    "    net.train(True)\n",
    "    valid_loss /= len(tk_valid) / 32\n",
    "    \n",
    "    for i, val_correct in enumerate(val_corrects):\n",
    "        val_corrects[i] = val_corrects[i] / len(tk_valid)\n",
    "        \n",
    "    return valid_loss, val_corrects, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1015it [00:08, 114.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss Variable containing:\n",
      " 0.5868\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "valid acc [0.9477999806404114, 0.9908999800682068, 0.9709999561309814, 0.9960999488830566, 0.9642999768257141, 0.9905999898910522, 0.946899950504303]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024it [00:17, 114.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss Variable containing:\n",
      " 0.5916\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "valid acc [0.949999988079071, 0.991599977016449, 0.9716999530792236, 0.9966999888420105, 0.9666999578475952, 0.9909999966621399, 0.9496999979019165]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3020it [00:26, 114.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss Variable containing:\n",
      " 0.6037\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "valid acc [0.9503999948501587, 0.9914000034332275, 0.9731999635696411, 0.9966999888420105, 0.9679999947547913, 0.9908999800682068, 0.9496999979019165]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4017it [00:35, 114.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss Variable containing:\n",
      " 0.5763\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "valid acc [0.9494999647140503, 0.9914000034332275, 0.9722999930381775, 0.9966999888420105, 0.9657999873161316, 0.9908999800682068, 0.9498999714851379]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4675it [00:40, 116.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_corrects = [0 for i in range(config.num_labels)]\n",
    "train_loss = 0\n",
    "\n",
    "for step, (batch, y_label) in tqdm(enumerate(batchify(tk_train, y_labels.values))):\n",
    "    \n",
    "    var_batch = TEXT.process(batch, device=0, train=True)\n",
    "    var_y = Variable(torch.cuda.LongTensor(y_label)).transpose(dim0=0, dim1=1)\n",
    "\n",
    "    pred_score = net(var_batch.transpose(dim0=0, dim1=1))\n",
    "    \n",
    "    net.zero_grad()\n",
    "    y_total_loss = 0\n",
    "    for i, score in enumerate(pred_score):\n",
    "        _, pred = score.max(dim=1)\n",
    "        train_corrects[i] += (pred == var_y[i]).float().sum()\n",
    "        y_loss = criterions[i](score, var_y[i])\n",
    "#         print(y_loss.data[0])\n",
    "        y_total_loss += y_loss\n",
    "    \n",
    "    y_total_loss.backward()\n",
    "    if step % 1000 == 999:\n",
    "        valid_loss, valid_acc, val_score = validation(net, tk_valid, y_valid, TEXT, criterions)\n",
    "        print(\"valid loss\", valid_loss)\n",
    "        print(\"valid acc\", [i.data[0] for i in valid_acc])\n",
    "\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_loss, valid_acc, val_score = validation(net, tk_valid, y_valid, TEXT, criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = [] # 모든 라벨의 validation set에 대한 예측값\n",
    "for i in range(config.num_labels):\n",
    "    prediction = [s[i] for s in val_score] # i번째 라벨에 대한 예측값을 배치에 따라 모음\n",
    "    predictions.append(torch.cat(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "---\n",
    "### roc_auc_score w.r.t. validation set's score\n",
    "- Kaggle form에 맞추어 column-wise roc auc score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941990985132\n",
      "0.977937669675\n",
      "0.963982988188\n",
      "0.936143820061\n",
      "0.95841659655\n",
      "0.898814889562\n"
     ]
    }
   ],
   "source": [
    "roc_auc_scores = 0\n",
    "for i in range(config.num_labels - 1): # minus 1 for normal \n",
    "    score = roc_auc_score( y_valid[labels[i]].values, predictions[i].numpy()[:, 1])\n",
    "    print(score)\n",
    "    roc_auc_scores += score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946214491528\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_scores / (config.num_labels - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_recsys]",
   "language": "python",
   "name": "conda-env-tf_recsys-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
