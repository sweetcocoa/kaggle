{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 len_sentence,\n",
    "                 channel_size=4,\n",
    "                 fc_dim=128,\n",
    "                 padding_idx=1,\n",
    "                 dropout=0.3,\n",
    "                 num_labels=7,\n",
    "                 batch_size=32,\n",
    "                ):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.channel_size = channel_size\n",
    "        self.len_sentence = len_sentence\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.convnet = nn.Sequential(OrderedDict(\n",
    "        [\n",
    "            ('layer 1', nn.Conv2d(1, out_channels=channel_size, kernel_size=3, stride=1, padding=0)), # input : 1 * embedding_dim * len_sentence => \n",
    "            ('relu 1', nn.LeakyReLU(inplace=True)), # output : channel_size * (embedding_dim-2) * (len_sentence-2)\n",
    "            ('avgpool 1', nn.AvgPool2d(2, stride=2)), # output : channel_size * (embedding_dim-2)/2 * (len_sentence-2)/2\n",
    "            ('batchnorm 1', nn.BatchNorm2d(channel_size)),\n",
    "            ('dropout 1', nn.Dropout2d(p=dropout, inplace=True)),\n",
    "            ('layer 2', nn.Conv2d(in_channels=channel_size, out_channels=channel_size*2, kernel_size=3, stride=1, padding=0)), \n",
    "            ('relu 2', nn.LeakyReLU(inplace=True)), \n",
    "            ('avgpool 2', nn.AvgPool2d(2, stride=2)), # output : channel_size * 2 * ((embedding_dim-2)/2 -2)/2 * (((len_sentence-2)/2)-2)/2  \n",
    "        ]))\n",
    "        \n",
    "        self.bottleneck_size = channel_size * ((embedding_dim-2)/2 -2) * (((len_sentence-2)/2)-2)/2 \n",
    "        print (\"Linear size : \", self.bottleneck_size)\n",
    "        assert self.bottleneck_size.is_integer()\n",
    "        self.bottleneck_size = int(self.bottleneck_size)\n",
    "        self.fcns1 = [nn.Linear(self.bottleneck_size, fc_dim) for i in range(num_labels)]\n",
    "        self.fcns2 = [nn.Linear(fc_dim, 2) for i in range(num_labels)]\n",
    "        self.fc_dim = fc_dim\n",
    "        self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, sentence, other_features=None):\n",
    "        \n",
    "        image = self.embedding(sentence)\n",
    "        \n",
    "        bottleneck = self.convnet(image)\n",
    "        print(bottleneck.shape)\n",
    "        bottleneck = bottleneck.view(-1, self.bottleneck_size)\n",
    "        \n",
    "        fcns_1 = []\n",
    "        for i in range(self.num_labels):\n",
    "            fcns_1.append(self.fcns1[i](bottleneck))\n",
    "        \n",
    "        fcns_2 = []\n",
    "        for i in range(self.num_labels):\n",
    "            fcns_2.append(self.fcns2[i](fcns_1[i]))\n",
    "            \n",
    "        return fcns_2 # return num_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 20000\n",
    "    embedding_dim = 50\n",
    "    len_sentence = 30\n",
    "    num_labels = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweetcocoa\\Anaconda3\\envs\\torch\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_pd_data('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_pd_data('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_capital_ratio(train), set_capital_ratio(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>alphas</th>\n",
       "      <th>capitals</th>\n",
       "      <th>cap_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>17</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>0.108108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "      <td>11</td>\n",
       "      <td>0.022587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  alphas  capitals  \\\n",
       "0             0        0       0       0              0     203        17   \n",
       "1             0        0       0       0              0      73         8   \n",
       "2             0        0       0       0              0     186         4   \n",
       "3             0        0       0       0              0     486        11   \n",
       "4             0        0       0       0              0      50         2   \n",
       "\n",
       "   cap_ratio  \n",
       "0   0.083333  \n",
       "1   0.108108  \n",
       "2   0.021390  \n",
       "3   0.022587  \n",
       "4   0.039216  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_train = train['comment_text'].str.lower().apply(tokenizer)\n",
    "tk_test = train['comment_text'].str.lower().apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,  \n",
    "                  # 들어갈 데이터가 sequential 인가요? 우리는 tokenize한 word의 sequence를 다룰거니까 True입니다. Defualt로도 True임.\n",
    "                  tokenize=tokenizer, \n",
    "                  # 그 데이터를 tokenize할 함수를 지정할 수 있습니다. 우리는 gensim library의 tokenize 함수를 쓸건데요\n",
    "                  # 뭐 굳이 그거 말고도 직접 정의해도 되고 str.split 같은걸 써넣어도 됩니다.\n",
    "                  # :: 그런 줄 알았는데 아무 tokenize 함수나 쓰면 안되고, generator가 아닌 tokenized list 를 반환하는 함수여야합니다..\n",
    "                  # :::: 이게 아닐거같기도 함.\n",
    "                  fix_length=20,\n",
    "                 # 아마 tokenize된 길이 제한 같은데 한번 확인해볼게요. 특이사항으로는 length 넘으면 자르고, 안넘으면 padding을 채웁니다\n",
    "                  # :: 그게 아니고 vector화 했을 때의 길이 제한일 것 같아요. 확인해보겠습니다.\n",
    "                  pad_first=True,\n",
    "                  # padding이 앞에서부터 붙냐, 뒤에서부터 붙냐는 겁니다.\n",
    "                  tensor_type=torch.cuda.LongTensor\n",
    "                  # cuda를 써도 됩니다\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(tk_train, tk_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [explanation, why, the, edits, made, under, my...\n",
       "1    [d, aww, he, matches, this, background, colour...\n",
       "2    [hey, man, i, m, really, not, trying, to, edit...\n",
       "3    [more, i, can, t, make, any, real, suggestions...\n",
       "4    [you, sir, are, my, hero, any, chance, you, re...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = TEXT.process(tk_train[:10], device=0, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 5 \n",
       " 6.6000e+02  1.0000e+00  4.0300e+02  6.2000e+01  1.0000e+00  1.0000e+00\n",
       " 7.8000e+01  1.0000e+00  4.1400e+02  4.0000e+00  1.0000e+00  1.0000e+00\n",
       " 2.0000e+00  1.0000e+00  4.0000e+00  3.8000e+01  1.0000e+00  1.0000e+00\n",
       " 1.3100e+02  1.0000e+00  7.1000e+01  2.3000e+01  1.0000e+00  1.0000e+00\n",
       " 1.3400e+02  1.5200e+02  1.3800e+02  1.0000e+02  1.0000e+00  1.0000e+00\n",
       " 1.7900e+02  1.5879e+04  1.5000e+01  5.8000e+01  1.0000e+00  1.0000e+00\n",
       " 3.2000e+01  5.2000e+01  2.4900e+02  3.3500e+02  7.0000e+00  1.0000e+00\n",
       " 6.4800e+02  2.5130e+03  3.0000e+00  1.3850e+03  1.6210e+03  1.0000e+00\n",
       " 4.3040e+03  1.4000e+01  7.7000e+01  1.6000e+01  2.1000e+01  1.0000e+00\n",
       " 1.0898e+04  5.4400e+02  3.1300e+02  2.0140e+03  3.2000e+01  1.0000e+00\n",
       " 9.9900e+02  3.6070e+03  1.1000e+01  4.0000e+00  3.2910e+03  2.5490e+03\n",
       " 8.8000e+01  4.0000e+00  1.9000e+01  5.7140e+03  5.8000e+01  3.4000e+01\n",
       " 3.2900e+02  7.1000e+01  5.5000e+01  2.5000e+01  1.0240e+03  3.9000e+01\n",
       " 5.0000e+01  4.3110e+03  1.0000e+01  2.0000e+00  7.0000e+00  1.8000e+01\n",
       " 2.1490e+03  2.5950e+03  1.4000e+01  1.2700e+02  5.6600e+02  1.0300e+02\n",
       " 2.3000e+01  2.4000e+01  5.8800e+02  2.3790e+03  4.3000e+01  8.4000e+01\n",
       " 1.1126e+04  9.7000e+01  9.0000e+00  6.0000e+01  3.1000e+01  2.0000e+00\n",
       " 5.5000e+01  4.0000e+01  2.1870e+03  1.7000e+01  1.0000e+01  2.1410e+03\n",
       " 6.5470e+03  9.3900e+02  4.8500e+02  5.0500e+02  1.9000e+01  1.0300e+02\n",
       " 1.6000e+01  1.8400e+02  4.9600e+02  1.6000e+01  1.6000e+01  4.0000e+01\n",
       "\n",
       "Columns 6 to 9 \n",
       " 1.0000e+00  2.2000e+01  2.6100e+02  1.0000e+00\n",
       " 1.0000e+00  2.0200e+02  2.5000e+01  1.0000e+00\n",
       " 1.0000e+00  3.0000e+00  2.0000e+00  1.0000e+00\n",
       " 1.0000e+00  2.0000e+00  2.8500e+02  1.0000e+00\n",
       " 1.0000e+00  3.2610e+03  7.4400e+02  1.0000e+00\n",
       " 1.0000e+00  1.5649e+05  2.7000e+01  1.0000e+00\n",
       " 1.0000e+00  2.6000e+01  1.4180e+03  1.0000e+00\n",
       " 1.0000e+00  4.6000e+01  3.0000e+00  1.0000e+00\n",
       " 1.0000e+00  5.9000e+01  7.0000e+00  1.0505e+04\n",
       " 1.0000e+00  3.2900e+02  4.8200e+02  1.6000e+01\n",
       " 1.0000e+00  4.8000e+01  4.0000e+00  1.4000e+01\n",
       " 1.0000e+00  6.1000e+01  7.1000e+01  2.2200e+02\n",
       " 1.7910e+03  2.3000e+01  1.5000e+01  6.0000e+00\n",
       " 1.5000e+02  3.6000e+01  8.8290e+03  5.7000e+01\n",
       " 7.0000e+00  1.1000e+01  3.0000e+00  2.1000e+01\n",
       " 3.2930e+03  1.3000e+02  3.3000e+02  1.7330e+03\n",
       " 3.2200e+02  2.8000e+01  2.1900e+02  3.0000e+00\n",
       " 1.6000e+01  7.0000e+00  1.2000e+01  1.4600e+02\n",
       " 3.2000e+01  4.7000e+01  2.0000e+00  5.0000e+00\n",
       " 1.4500e+02  1.7000e+01  2.6000e+01  5.1619e+04\n",
       "[torch.cuda.LongTensor of size 20x10 (GPU 0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/train.csv')\n",
    "test_set = pd.read_csv('./data/test.csv')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize, simple_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.2\n",
    "train_num = int(len(train_set) * (1-valid_ratio))\n",
    "train_datasets = train_set.loc[0:train_num]\n",
    "valid_datasets = train_set.loc[train_num:]\n",
    "valid_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets.to_csv('./data/train_cached.csv', index=False)\n",
    "valid_datasets.to_csv('./data/valid_cached.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,  \n",
    "                  # 들어갈 데이터가 sequential 인가요? 우리는 tokenize한 word의 sequence를 다룰거니까 True입니다. Defualt로도 True임.\n",
    "                  tokenize=tokenizer, \n",
    "                  # 그 데이터를 tokenize할 함수를 지정할 수 있습니다. 우리는 gensim library의 tokenize 함수를 쓸건데요\n",
    "                  # 뭐 굳이 그거 말고도 직접 정의해도 되고 str.split 같은걸 써넣어도 됩니다.\n",
    "                  # :: 그런 줄 알았는데 아무 tokenize 함수나 쓰면 안되고, generator가 아닌 tokenized list 를 반환하는 함수여야합니다..\n",
    "                  # :::: 이게 아닐거같기도 함.\n",
    "                  fix_length=20,\n",
    "                 # 아마 tokenize된 길이 제한 같은데 한번 확인해볼게요. 특이사항으로는 length 넘으면 자르고, 안넘으면 padding을 채웁니다\n",
    "                  # :: 그게 아니고 vector화 했을 때의 길이 제한일 것 같아요. 확인해보겠습니다.\n",
    "                  pad_first=True,\n",
    "                  # padding이 앞에서부터 붙냐, 뒤에서부터 붙냐는 겁니다.\n",
    "                  tensor_type=torch.LongTensor\n",
    "                  # cuda를 써도 됩니다\n",
    "                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_capital_ratio(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = train_set['comment_text'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, val, # 몇 개의 단어 셋을 줘도 괜찮습니다.\n",
    "                 max_size=config.vocab_size,  # 단어사전의 최대 크기입니다. 여기에는 padding word, unknown word는 포함되지 않습니다.\n",
    "                 min_freq=3  # 몇 개 이상 나타나는 단어에 대해서만 vocab을 생성합니다. \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(vocab_size=config.vocab_size, embedding_dim=config.embedding_dim, len_sentence=config.len_sentence, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((20, 1, 50, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(Variable(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
